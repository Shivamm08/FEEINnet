{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb8c116f-1d2a-442c-ab51-b5f99e16ebc5",
   "metadata": {},
   "source": [
    "## SECTION A: Imports, Seed Setup, and File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051e66ee-73d0-4eab-bf86-fd1516c7b4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION A - Imports and Setup\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_ssim\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# === Define Paths ===\n",
    "base_path = r\"D:\\Datasets\\ham10000\"  \n",
    "\n",
    "train_image_dir = os.path.join(base_path, 'images/train')\n",
    "val_image_dir = os.path.join(base_path, 'images/val')\n",
    "test_image_dir = os.path.join(base_path, 'images/test')\n",
    "\n",
    "train_mask_dir = os.path.join(base_path, 'annotations/train')\n",
    "val_mask_dir = os.path.join(base_path, 'annotations/val')\n",
    "test_mask_dir = os.path.join(base_path, 'annotations/test')\n",
    "\n",
    "# Read image IDs\n",
    "def read_ids(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        return [line.strip() for line in f.readlines()]\n",
    "\n",
    "train_ids = read_ids(os.path.join(base_path, 'train.txt'))\n",
    "val_ids = read_ids(os.path.join(base_path, 'val.txt'))\n",
    "test_ids = read_ids(os.path.join(base_path, 'test.txt'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8399adc4-3e8f-4f21-ae2c-af45dffa5733",
   "metadata": {},
   "source": [
    "## SECTION B: PyTorch Dataset Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a4f376-a880-4b4e-a258-5e73d47acd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION B - Custom Dataset Class\n",
    "\n",
    "class SkinLesionDataset(Dataset):\n",
    "    def __init__(self, ids_list, image_dir, mask_dir, image_size=(384, 384), augment=False):\n",
    "        self.ids_list = ids_list\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.image_size = image_size\n",
    "        self.augment = augment\n",
    "        \n",
    "        # Transformations\n",
    "        self.transform_image = transforms.Compose([\n",
    "            transforms.Resize(image_size, interpolation=Image.BICUBIC),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        self.transform_mask = transforms.Compose([\n",
    "            transforms.Resize(image_size, interpolation=Image.NEAREST),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.ids_list[idx]\n",
    "\n",
    "        image_path = os.path.join(self.image_dir, image_id + \".jpg\")\n",
    "        mask_path = os.path.join(self.mask_dir, image_id + \".png\")\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        image = self.transform_image(image)\n",
    "        mask = self.transform_mask(mask)\n",
    "        \n",
    "        # Binarize mask\n",
    "        mask = (mask >= 0.5).float()\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def get_filenames(self, idx):\n",
    "        batch_ids = self.ids_list[idx]\n",
    "        return batch_ids\n",
    "\n",
    "# Create Dataset Instances\n",
    "train_dataset = SkinLesionDataset(train_ids, train_image_dir, train_mask_dir, image_size=(384, 384))\n",
    "val_dataset = SkinLesionDataset(val_ids, val_image_dir, val_mask_dir, image_size=(384, 384))\n",
    "test_dataset = SkinLesionDataset(test_ids, test_image_dir, test_mask_dir, image_size=(384, 384))\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02bd2a7-ff89-4abd-b15f-21e5bb690292",
   "metadata": {},
   "source": [
    "## SECTION C: Custom Loss Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a34b48c-98f4-4eff-8088-e400976dd5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-5):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred = torch.sigmoid(y_pred)\n",
    "        y_pred = y_pred.view(-1)\n",
    "        y_true = y_true.view(-1)\n",
    "        \n",
    "        intersection = (y_pred * y_true).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (y_pred.sum() + y_true.sum() + self.smooth)\n",
    "        return 1 - dice\n",
    "\n",
    "class BinaryFocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=0.25):\n",
    "        super(BinaryFocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred = torch.sigmoid(y_pred)\n",
    "        y_true = y_true.view(-1)\n",
    "        y_pred = y_pred.view(-1)\n",
    "\n",
    "        bce = nn.BCELoss(reduction='none')(y_pred, y_true)\n",
    "        pt = torch.exp(-bce)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce\n",
    "        return focal_loss.mean()\n",
    "\n",
    "\n",
    "\n",
    "class SSIMLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SSIMLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred = torch.sigmoid(y_pred)\n",
    "        ssim_score = pytorch_ssim.ssim(y_pred, y_true)\n",
    "        return 1 - ssim_score\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.dice = DiceLoss()\n",
    "        self.focal = BinaryFocalLoss()\n",
    "        self.ssim = SSIMLoss()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss_dice = self.dice(y_pred, y_true)\n",
    "        loss_focal = self.focal(y_pred, y_true)\n",
    "        loss_ssim = self.ssim(y_pred, y_true)\n",
    "        return loss_dice + loss_focal + loss_ssim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5efa07-a4d5-4c22-8ac5-d9adb8a0ec0b",
   "metadata": {},
   "source": [
    "## SECTION D: Implementation of FEEINnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c13ab8-e0ce-494e-8c47-b7930040b661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels, out_channels, dropout=0.5):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.upsample = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout2d(dropout)\n",
    "        self.conv = ConvBlock(out_channels + skip_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.dropout(x)\n",
    "        return self.conv(x)\n",
    "\n",
    "class channel_attention(nn.Module):\n",
    "    def __init__(self, in_channels, ratio=8):\n",
    "        super(channel_attention, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.shared_fc1 = nn.Linear(in_channels, in_channels // ratio)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.shared_fc2 = nn.Linear(in_channels // ratio, in_channels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def squeeze_operation(self, x):\n",
    "        # x: (B, C, H, W) â†’ (B, C)\n",
    "        B, C, H, W = x.size()\n",
    "        return x.view(B, C, -1).sum(dim=2) / (H * W)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "\n",
    "        # Global Average Pooling\n",
    "        avg_pool = F.adaptive_avg_pool2d(x, 1)\n",
    "        avg_pool = self.squeeze_operation(avg_pool)\n",
    "        avg_out = self.shared_fc1(avg_pool)\n",
    "        avg_out = self.relu(avg_out)\n",
    "        avg_out = self.shared_fc2(avg_out)\n",
    "\n",
    "        # Global Max Pooling\n",
    "        max_pool = F.adaptive_max_pool2d(x, 1)\n",
    "        max_pool = self.squeeze_operation(max_pool)\n",
    "        max_out = self.shared_fc1(max_pool)\n",
    "        max_out = self.relu(max_out)\n",
    "        max_out = self.shared_fc2(max_out)\n",
    "\n",
    "        # Combine + sigmoid\n",
    "        combined = avg_out + max_out\n",
    "        attn = self.sigmoid(combined).view(B, C, 1, 1)\n",
    "\n",
    "        return x * attn.expand_as(x)\n",
    "\n",
    "class EdgeAttentionModule(nn.Module):\n",
    "    def __init__(self, num_filters):\n",
    "        super(EdgeAttentionModule, self).__init__()\n",
    "        # For s3 and s4\n",
    "        self.conv_s3 = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_filters),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.conv_s4 = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_filters),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        # For combined\n",
    "        self.conv_combined = nn.Sequential(\n",
    "            nn.Conv2d(num_filters * 2, num_filters, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_filters),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        # For final decoder output\n",
    "        self.conv_final_map = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_filters),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        # After multiplication\n",
    "        self.conv_enhanced = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_filters),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "    def distance_transform(self, S):\n",
    "        S = 1 - S\n",
    "        kernel = torch.ones((S.shape[1], S.shape[1], 5, 5), device=S.device)\n",
    "        S = F.conv2d(S, kernel, stride=1, padding=2, groups=S.shape[1])\n",
    "        return S\n",
    "\n",
    "    def calculate_di(self, Si, inv_Si):\n",
    "        DT_Si = self.distance_transform(Si)\n",
    "        DT_inv_Si = self.distance_transform(inv_Si)\n",
    "\n",
    "        max_DT_Si = DT_Si.amax(dim=(2, 3), keepdim=True)\n",
    "        max_DT_inv_Si = DT_inv_Si.amax(dim=(2, 3), keepdim=True)\n",
    "\n",
    "        Di = (DT_Si / (max_DT_Si + 1e-8)) + (DT_inv_Si / (max_DT_inv_Si + 1e-8))\n",
    "        return Di\n",
    "\n",
    "    def forward(self, s3, s4, final_decoder_output):\n",
    "        s3 = self.conv_s3(s3)\n",
    "        s4 = F.interpolate(s4, size=s3.shape[2:], mode='bilinear', align_corners=False)\n",
    "        s4 = self.conv_s4(s4)\n",
    "\n",
    "        combined = torch.cat([s3, s4], dim=1)\n",
    "        combined = self.conv_combined(combined)\n",
    "\n",
    "        final_map = self.conv_final_map(final_decoder_output)\n",
    "\n",
    "        boundary = torch.sigmoid(final_map)\n",
    "        inv_boundary = 1 - boundary\n",
    "\n",
    "        Di = self.calculate_di(boundary, inv_boundary)\n",
    "        Mi_B = 1 - Di\n",
    "\n",
    "        combined_up = F.interpolate(combined, size=Mi_B.shape[2:], mode='bilinear', align_corners=False)\n",
    "        enhanced_boundary = combined_up * Mi_B\n",
    "        enhanced_boundary = self.conv_enhanced(enhanced_boundary)\n",
    "\n",
    "        return enhanced_boundary\n",
    "\n",
    "class inverse_attention_module(nn.Module):\n",
    "    def __init__(self, Fi_channels, Ob_channels, num_filters):\n",
    "        super(inverse_attention_module, self).__init__()\n",
    "\n",
    "        # 1x1 convolution for downsampling Ob to match Fi channels\n",
    "        self.down_ob = nn.Sequential(\n",
    "            nn.Conv2d(Ob_channels, num_filters, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(num_filters),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        # Convolutional block for Fi + Ob\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(Fi_channels + num_filters, num_filters, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_filters),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_filters),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, Ui_plus_1, Fi, Ob):\n",
    "        B, C, H, W = Fi.shape\n",
    "        _, _, H_ob, W_ob = Ob.shape\n",
    "\n",
    "        # Compute downsampling factors for Ob\n",
    "        factor_h = max(H_ob // H, 1)\n",
    "        factor_w = max(W_ob // W, 1)\n",
    "\n",
    "        # Downsample Ob using stride to match Fi resolution\n",
    "        Ob_down = F.conv2d(\n",
    "            Ob,\n",
    "            weight=self.down_ob[0].weight,\n",
    "            bias=self.down_ob[0].bias,\n",
    "            stride=(factor_h, factor_w),\n",
    "            padding=0\n",
    "        )\n",
    "        Ob_down = self.down_ob[1](Ob_down)\n",
    "        Ob_down = self.down_ob[2](Ob_down)\n",
    "\n",
    "        # Concatenate Fi and downsampled Ob\n",
    "        Fi_concat = torch.cat([Fi, Ob_down], dim=1)\n",
    "\n",
    "        # Convolution block to reduce channels\n",
    "        Fi_concat_conv = self.conv(Fi_concat)\n",
    "\n",
    "        # Sigmoid on Ui_plus_1\n",
    "        Si = torch.sigmoid(Ui_plus_1)\n",
    "        inv_Si = 1 - Si\n",
    "\n",
    "        # Upsample Fi_concat_conv to match Ui_plus_1\n",
    "        Fi_concat_conv_up = F.interpolate(Fi_concat_conv, size=Ui_plus_1.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Element-wise multiplication\n",
    "        refined_feature = Fi_concat_conv_up * inv_Si\n",
    "\n",
    "        return refined_feature\n",
    "\n",
    "class EfficientNetB4Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EfficientNetB4Encoder, self).__init__()\n",
    "        self.backbone = EfficientNet.from_pretrained('efficientnet-b4')\n",
    "\n",
    "        self.outputs = {}\n",
    "        self.feature_layers = {\n",
    "            \"s1\": \"_blocks.1\",   # stem\n",
    "            \"s2\": \"_blocks.2\",   # block1\n",
    "            \"s3\": \"_blocks.3\",   # block2\n",
    "            \"s4\": \"_blocks.4\",   # block3\n",
    "            \"s5\": \"_blocks.6\",   # block4\n",
    "            \"b1\": \"_blocks.7\"    # bottleneck\n",
    "        }\n",
    "\n",
    "        for name, layer_name in self.feature_layers.items():\n",
    "            layer = dict([*self.backbone.named_modules()])[layer_name]\n",
    "            layer.register_forward_hook(self.save_output(name))\n",
    "\n",
    "    def save_output(self, name):\n",
    "        def hook(module, input, output):\n",
    "            self.outputs[name] = output\n",
    "        return hook\n",
    "\n",
    "    def forward(self, x):\n",
    "        _ = self.backbone.extract_features(x)\n",
    "        return [self.outputs[\"s1\"], self.outputs[\"s2\"], self.outputs[\"s3\"],\n",
    "                self.outputs[\"s4\"], self.outputs[\"s5\"], self.outputs[\"b1\"]]\n",
    "\n",
    "\n",
    "class FEEINnet(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(FEEINnet, self).__init__()\n",
    "        self.encoder = EfficientNetB4Encoder()\n",
    "        self.s1_conv = nn.Conv2d(in_channels=48, out_channels=48, kernel_size=1)\n",
    "        self.s2_conv = nn.Conv2d(in_channels=64, out_channels=144, kernel_size=1)\n",
    "        self.s3_conv = nn.Conv2d(in_channels=160, out_channels=192, kernel_size=1)\n",
    "        self.s4_conv = nn.Conv2d(in_channels=224, out_channels=336, kernel_size=1)\n",
    "        self.s5_conv = nn.Conv2d(in_channels=384, out_channels=1632, kernel_size=1)\n",
    "\n",
    "        self.b1_conv = nn.Conv2d(in_channels=1792, out_channels=1632, kernel_size=1)\n",
    "\n",
    "        self.d1 = DecoderBlock(in_channels=1632, skip_channels=336, out_channels=256, dropout=dropout_rate)\n",
    "        self.cam1 = channel_attention(256)\n",
    "\n",
    "        self.d2 = DecoderBlock(256, 192, 128, dropout_rate)\n",
    "        self.d3 = DecoderBlock(128, 144, 64, dropout_rate)\n",
    "        self.cam3 = channel_attention(64)\n",
    "\n",
    "        self.d4 = DecoderBlock(64, 48, 32, dropout_rate)\n",
    "        self.d5 = DecoderBlock(32, 3, 16, dropout_rate)  \n",
    "        self.cam5 = channel_attention(16)\n",
    "\n",
    "        # EAM\n",
    "        self.eam = EdgeAttentionModule(16)\n",
    "\n",
    "        # IAM modules\n",
    "        self.iam1 = inverse_attention_module(Fi_channels=1632, Ob_channels=16, num_filters=16)\n",
    "        self.iam2 = inverse_attention_module(Fi_channels=1632, Ob_channels=16, num_filters=16)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(16, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        s1, s2, s3, s4, s5, b1 = self.encoder(x)\n",
    "\n",
    "        s1 = self.s1_conv(s1)\n",
    "        s2 = self.s2_conv(s2)\n",
    "        s3 = self.s3_conv(s3)\n",
    "        s4 = self.s4_conv(s4)\n",
    "        s5 = self.s5_conv(s5)\n",
    "        b1 = self.b1_conv(b1)\n",
    "\n",
    "        # Decoder\n",
    "        d1 = self.cam1(self.d1(b1, s5))\n",
    "        d2 = self.d2(d1, s4)\n",
    "        d3 = self.cam3(self.d3(d2, s3))\n",
    "        d4 = self.d4(d3, s2)\n",
    "        d5 = self.cam5(self.d5(d4, s1))\n",
    "\n",
    "        # Edge Attention\n",
    "        enhanced_boundary = self.eam(s3, s4, d5)\n",
    "\n",
    "        # Inverse Attention Mechanism\n",
    "        ra1_out = self.iam1(d5, b1, enhanced_boundary)\n",
    "        ra1_sum = ra1_out + d5\n",
    "\n",
    "        ra2_out = self.iam2(ra1_sum, s5, enhanced_boundary)\n",
    "        ra2_sum = ra2_out + ra1_sum\n",
    "\n",
    "        output = torch.sigmoid(self.final_conv(ra2_sum))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d3ee7b-ccfa-4145-95a9-42020f22b411",
   "metadata": {},
   "source": [
    "## SECTION E: Optimizer and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cca32b8-6302-429a-9055-33fd70a21254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image dimensions\n",
    "IMAGE_WIDTH, IMAGE_HEIGHT, CHANNELS = 384, 384, 3\n",
    "\n",
    "# Initialize model\n",
    "model = FEEINnet(dropout_rate=0.4).to(device)\n",
    "print(model)\n",
    "\n",
    "# Loss\n",
    "loss_fn = CombinedLoss()\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n",
    "\n",
    "# Scheduler (ReduceLROnPlateau)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.1, patience=5, min_lr=1e-6, verbose=True\n",
    ")\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "num_epochs = 100\n",
    "early_stopping = EarlyStopping(patience=10)\n",
    "\n",
    "train_history = {\n",
    "    \"loss\": [], \"val_loss\": [],\n",
    "    \"iou\": [], \"dice\": [],\n",
    "    \"precision\": [], \"recall\": [],\n",
    "    \"accuracy\": [], \"ssim\": []\n",
    "}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, masks in train_loader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_iou, all_dice, all_prec, all_rec, all_acc, all_ssim = [], [], [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            all_iou.append(iou_score(outputs, masks))\n",
    "            all_dice.append(dice_score(outputs, masks))\n",
    "            all_prec.append(precision_score(outputs, masks))\n",
    "            all_rec.append(recall_score(outputs, masks))\n",
    "            all_acc.append(accuracy_score(outputs, masks))\n",
    "            all_ssim.append(ssim_score(outputs, masks))\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    metrics = {\n",
    "        \"iou\": np.mean(all_iou),\n",
    "        \"dice\": np.mean(all_dice),\n",
    "        \"precision\": np.mean(all_prec),\n",
    "        \"recall\": np.mean(all_rec),\n",
    "        \"accuracy\": np.mean(all_acc),\n",
    "        \"ssim\": np.mean(all_ssim)\n",
    "    }\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "          f\"IoU: {metrics['iou']:.4f} | Dice: {metrics['dice']:.4f} | \"\n",
    "          f\"Precision: {metrics['precision']:.4f} | Recall: {metrics['recall']:.4f} | \"\n",
    "          f\"Accuracy: {metrics['accuracy']:.4f} | SSIM: {metrics['ssim']:.4f}\")\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Save history\n",
    "    train_history[\"loss\"].append(train_loss)\n",
    "    train_history[\"val_loss\"].append(val_loss)\n",
    "    for key in [\"iou\", \"dice\", \"precision\", \"recall\", \"accuracy\", \"ssim\"]:\n",
    "        train_history[key].append(metrics[key])\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss <= early_stopping.best_loss:\n",
    "        torch.save(model.state_dict(), \"best_feeinnet.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b4968f-5d54-484e-a5d5-f14f2ac29f47",
   "metadata": {},
   "source": [
    "## SECTION F: Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c397000-55a3-41b5-9966-2f211e088c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload best saved model\n",
    "model.load_state_dict(torch.load(\"best_feeinnet.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate(model, dataloader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_iou, all_dice, all_prec, all_rec, all_acc, all_ssim = [], [], [], [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, masks)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_iou.append(iou_score(outputs, masks))\n",
    "            all_dice.append(dice_score(outputs, masks))\n",
    "            all_prec.append(precision_score(outputs, masks))\n",
    "            all_rec.append(recall_score(outputs, masks))\n",
    "            all_acc.append(accuracy_score(outputs, masks))\n",
    "            all_ssim.append(ssim_score(outputs, masks))\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    metrics = {\n",
    "        \"loss\": avg_loss,\n",
    "        \"iou\": np.mean(all_iou),\n",
    "        \"fscore\": np.mean(all_dice),  # F1 = Dice\n",
    "        \"accuracy\": np.mean(all_acc),\n",
    "        \"precision\": np.mean(all_prec),\n",
    "        \"recall\": np.mean(all_rec),\n",
    "        \"ssim\": np.mean(all_ssim),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# Validation Evaluation\n",
    "val_metrics = evaluate(model, val_loader, loss_fn)\n",
    "print(\"Validation Results:\")\n",
    "print(f\"Validation Loss: {val_metrics['loss']:.4f}\")\n",
    "print(f\"Validation IOU: {val_metrics['iou']:.4f}\")\n",
    "print(f\"Validation FScore: {val_metrics['fscore']:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "print(f\"Validation Precision: {val_metrics['precision']:.4f}\")\n",
    "print(f\"Validation Recall: {val_metrics['recall']:.4f}\")\n",
    "print(f\"Validation SSIM: {val_metrics['ssim']:.4f}\")\n",
    "\n",
    "# Test Evaluation\n",
    "test_metrics = evaluate(model, test_loader, loss_fn)\n",
    "print(\"\\nTest Results:\")\n",
    "print(f\"Test Loss: {test_metrics['loss']:.4f}\")\n",
    "print(f\"Test IOU: {test_metrics['iou']:.4f}\")\n",
    "print(f\"Test FScore: {test_metrics['fscore']:.4f}\")\n",
    "print(f\"Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"Test Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"Test Recall: {test_metrics['recall']:.4f}\")\n",
    "print(f\"Test SSIM: {test_metrics['ssim']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9075d958-e195-4d41-83d5-552f7e344e5f",
   "metadata": {},
   "source": [
    "## SECTION G: Visualize Training and Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bc290a-0106-4eeb-88d8-cc93337657ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save outputs\n",
    "output_dir = r\"D:\\20BPS1134\\Implementation\\RA\\3\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Plot Training & Validation Loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_history[\"loss\"], label=\"Train Loss\")\n",
    "plt.plot(train_history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.title(\"Model Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(output_dir, \"loss.png\"))\n",
    "plt.show()\n",
    "\n",
    "# Plot Training & Validation IoU\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_history[\"iou\"], label=\"Train IoU\")\n",
    "plt.plot(train_history[\"val_loss\"], label=\"Validation IoU\")  # bug fix: should use \"iou\"\n",
    "plt.title(\"Model IoU Score\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"IoU Score\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(output_dir, \"iou_score.png\"))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a1367a-68ed-4030-a4a2-62c503057f36",
   "metadata": {},
   "source": [
    "## SECTION H: Visualize Results for 3 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2f4fa7-3542-404b-9eee-898e80c4dee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Function to load and preprocess image\n",
    "def load_image(image_path):\n",
    "    transform = T.Compose([\n",
    "        T.Resize((384, 384)),\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img = transform(img)\n",
    "    return img\n",
    "\n",
    "# Function to load mask\n",
    "def load_mask(mask_path):\n",
    "    transform = T.Compose([\n",
    "        T.Resize((384, 384)),\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    mask = Image.open(mask_path).convert(\"L\")\n",
    "    mask = transform(mask)\n",
    "    mask = (mask >= 0.5).float()\n",
    "    return mask\n",
    "\n",
    "# Function to visualize predictions\n",
    "def plot_predictions(model, image_tensor, mask_tensor, output_dir, img_index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image_input = image_tensor.unsqueeze(0).to(device)  # Add batch dimension\n",
    "        pred_mask = model(image_input)\n",
    "        pred_mask = torch.sigmoid(pred_mask).cpu().numpy()[0, 0]\n",
    "        pred_mask = np.round(pred_mask)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "    ax[0].imshow(np.transpose(image_tensor.numpy(), (1, 2, 0)))\n",
    "    ax[0].set_title('Input Image')\n",
    "    ax[1].imshow(mask_tensor.squeeze().numpy(), cmap='gray')\n",
    "    ax[1].set_title('True Mask')\n",
    "    ax[2].imshow(pred_mask, cmap='gray')\n",
    "    ax[2].set_title('Predicted Mask')\n",
    "\n",
    "    result_path = os.path.join(output_dir, f'result_{img_index}.png')\n",
    "    plt.savefig(result_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# Output directory\n",
    "output_dir = r\"D:\\20BPS1134\\Implementation\\RA\\3\\HAM10000_Results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Visualize predictions for 3 random test images\n",
    "for i in range(1, 4):\n",
    "    sample_index = np.random.choice(len(test_ids), 1, replace=False)[0]\n",
    "    sample_image_path = os.path.join(test_image_dir, test_ids[sample_index] + '.jpg')\n",
    "    sample_mask_path = os.path.join(test_mask_dir, test_ids[sample_index] + '.png')\n",
    "\n",
    "    sample_image = load_image(sample_image_path)\n",
    "    sample_mask = load_mask(sample_mask_path)\n",
    "\n",
    "    plot_predictions(model, sample_image, sample_mask, output_dir, img_index=i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
